{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewghannoum/miniconda3/envs/py310/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:51: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "import numpy as np\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that reads json file\n",
    "def read_json(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_custom(\n",
    "    http_host=\"localhost\",\n",
    "    http_port=8080,\n",
    "    http_secure=False,\n",
    "    grpc_host=\"localhost\",\n",
    "    grpc_port=50051,\n",
    "    grpc_secure=False,\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(\n",
    "        config[\"AUTHENTICATION_APIKEY_ALLOWED_KEYS\"]\n",
    "    ),  # Set this environment variable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_regenerate = True\n",
    "\n",
    "if is_regenerate:\n",
    "    client.collections.delete(\"Subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_subject_collection = False\n",
    "subjects = None\n",
    "\n",
    "for collection in list(client.collections.list_all().keys()):\n",
    "    if collection == \"Subject\":\n",
    "        is_subject_collection = True\n",
    "\n",
    "if is_subject_collection:\n",
    "    subjects = client.collections.get(\"Subject\")\n",
    "else:\n",
    "    subjects = client.collections.create(\n",
    "        \"Subject\",\n",
    "        vectorizer_config=wvc.config.Configure.Vectorizer.none(),\n",
    "        vector_index_config=wvc.config.Configure.VectorIndex.hnsw(\n",
    "            distance_metric=wvc.config.VectorDistances.COSINE  # select prefered distance metric\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data_loaded = False\n",
    "\n",
    "for item in subjects.iterator():\n",
    "    is_data_loaded = True\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_data_loaded:\n",
    "    subject_objs = []\n",
    "\n",
    "    for filename in os.listdir(\"./document_embeddings/word2vec_mean\"):\n",
    "        if filename.endswith(\".json\"):\n",
    "            subject_embedding = read_json(\n",
    "                f\"./document_embeddings/word2vec_mean/{filename}\"\n",
    "            )\n",
    "            subject_code = filename.split(\".\")[0]\n",
    "\n",
    "            subject_objs.append(\n",
    "                wvc.data.DataObject(\n",
    "                    properties={\n",
    "                        \"subjectCode\": subject_code,\n",
    "                    },\n",
    "                    vector=subject_embedding,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    subjects.data.insert_many(subject_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_vector(data_point_index, vocab_size):\n",
    "    one_hot_vector = np.zeros(vocab_size)\n",
    "    one_hot_vector[data_point_index] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(query: str):\n",
    "  tokens = nltk.word_tokenize(query)\n",
    "  tokens = [word.lower() for word in tokens if word.isalpha()] #  and len(word) > 1\n",
    "  vocab = {\"<pad>\": 0} | {word: i+1 for i, word in enumerate(set(tokens))}\n",
    "  vocab_size = len(vocab)\n",
    "  \n",
    "  print(tokens)\n",
    "  \n",
    "  train_samples = []\n",
    "\n",
    "  window_size = 2\n",
    "\n",
    "  for i in range(window_size, len(tokens) - window_size):\n",
    "      for j in range(1, window_size + 1):\n",
    "          train_samples.append((tokens[i], tokens[i-j]))\n",
    "          train_samples.append((tokens[i], tokens[i+j]))\n",
    "          \n",
    "  print(\"train_samples\", train_samples)\n",
    "          \n",
    "  x_train = []\n",
    "  y_train = []\n",
    "\n",
    "  for word, target_word in train_samples:\n",
    "    x_train.append(vocab[word])\n",
    "    y_train.append(get_one_hot_vector(vocab[target_word], vocab_size))\n",
    "    \n",
    "  print(x_train)\n",
    "  print(y_train)\n",
    "    \n",
    "  x_train = np.asarray(x_train)\n",
    "  y_train = np.asarray(y_train)\n",
    "  \n",
    "  # Build the Word2Vec model using TensorFlow\n",
    "  embedding_dim = 100  # Adjust the dimensionality as needed\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "  \n",
    "  # Train the Word2Vec model\n",
    "  num_epochs = 10  # Adjust the number of epochs as needed\n",
    "\n",
    "  model.fit(x_train, y_train, epochs=num_epochs)\n",
    "  \n",
    "  word_embeddings = model.layers[0].get_weights()[0]\n",
    "      \n",
    "  return np.mean(word_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'algorithm', 'implementation', 'with', 'presentation', 'at', 'the', 'end', 'of', 'semester']\n",
      "train_samples [('algorithm', 'learning'), ('algorithm', 'implementation'), ('algorithm', 'machine'), ('algorithm', 'with'), ('implementation', 'algorithm'), ('implementation', 'with'), ('implementation', 'learning'), ('implementation', 'presentation'), ('with', 'implementation'), ('with', 'presentation'), ('with', 'algorithm'), ('with', 'at'), ('presentation', 'with'), ('presentation', 'at'), ('presentation', 'implementation'), ('presentation', 'the'), ('at', 'presentation'), ('at', 'the'), ('at', 'with'), ('at', 'end'), ('the', 'at'), ('the', 'end'), ('the', 'presentation'), ('the', 'of'), ('end', 'the'), ('end', 'of'), ('end', 'at'), ('end', 'semester')]\n",
      "[10, 10, 10, 10, 11, 11, 11, 11, 1, 1, 1, 1, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 4, 4, 4, 4]\n",
      "[array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])]\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 2.4895\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.4834\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4772\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4711\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4650\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4590\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.4529\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4469\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4408\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4348\n"
     ]
    }
   ],
   "source": [
    "query_embedding = get_query_embedding(\"Machine Learning algorithm implementation with presentation at the end of semester\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00966912,  0.01076559,  0.00152563, -0.02483802, -0.0232534 ,\n",
       "        0.00537849, -0.0041363 ,  0.00763978,  0.0116906 ,  0.00137249,\n",
       "        0.00061048, -0.00494826, -0.01506594,  0.00802219, -0.0135569 ,\n",
       "       -0.01018921, -0.00572379, -0.00051977,  0.0109072 ,  0.0001546 ,\n",
       "        0.01229581,  0.00207729,  0.0148156 , -0.00757831,  0.00922092,\n",
       "       -0.00026447, -0.00368129, -0.00752631,  0.00791247, -0.00446834,\n",
       "        0.00412131,  0.01322216,  0.01811293, -0.00446214,  0.00697161,\n",
       "       -0.00640676,  0.00641331,  0.00771248,  0.0053491 ,  0.00446344,\n",
       "       -0.00267432, -0.00848032, -0.00351101,  0.00166317,  0.00916756,\n",
       "        0.00542568, -0.00524739, -0.01105762, -0.00564059,  0.00335803,\n",
       "        0.00657216,  0.00056832, -0.01362528, -0.00043372, -0.00739061,\n",
       "        0.00240722, -0.00163874, -0.01134397,  0.00399193, -0.01872429,\n",
       "        0.00622769, -0.00318243, -0.00153846,  0.02161304, -0.01646972,\n",
       "        0.00713221,  0.00656816,  0.00596208,  0.00036124, -0.00732721,\n",
       "        0.01311136, -0.00508144, -0.01134968,  0.00355114, -0.00195313,\n",
       "        0.00434898, -0.00550289, -0.00966066, -0.00077773,  0.00839976,\n",
       "        0.01110404, -0.01479809, -0.02183183, -0.00552174, -0.00867812,\n",
       "        0.00558427, -0.01492084,  0.01013566,  0.01034387,  0.0053872 ,\n",
       "        0.00563323,  0.02373422,  0.00123914, -0.00901804, -0.00985085,\n",
       "        0.00314343,  0.00842304, -0.00721613, -0.0120227 ,  0.00059215],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryReturn(objects=[Object(uuid=_WeaviateUUIDInt('48ae18a4-9d4f-419c-a7f6-f4a039f432e6'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=0.6560970544815063, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'subjectCode': '43022'}, references=None, vector={}, collection='Subject'), Object(uuid=_WeaviateUUIDInt('6012e083-27eb-40ec-b503-bd9a18add807'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=0.6346902251243591, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'subjectCode': '42095'}, references=None, vector={}, collection='Subject'), Object(uuid=_WeaviateUUIDInt('22492e6d-90d1-4b4e-ab63-348b29fafa56'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=0.6285211443901062, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'subjectCode': '48572'}, references=None, vector={}, collection='Subject'), Object(uuid=_WeaviateUUIDInt('7b5be1f7-61c5-4834-bb6a-31c225a5ac1a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=0.6237137317657471, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'subjectCode': '31266'}, references=None, vector={}, collection='Subject'), Object(uuid=_WeaviateUUIDInt('9460d078-a799-4868-8611-caf1d52f6d12'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=0.5914880633354187, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'subjectCode': '41380'}, references=None, vector={}, collection='Subject')])\n"
     ]
    }
   ],
   "source": [
    "response = subjects.query.near_vector(\n",
    "    near_vector=query_embedding.tolist(),\n",
    "    limit=5,\n",
    "    return_metadata=wvc.query.MetadataQuery(certainty=True)\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['43022', '42095', '48572', '31266', '41380']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.properties[\"subjectCode\"] for x in response.objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()  # Close client gracefully"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
