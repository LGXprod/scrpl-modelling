{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "import spacy\n",
    "import pytextrank\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matthewghannoum/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytextrank.base.BaseTextRankFactory at 0x3005f4070>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text) -> str:\n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove line breaks\n",
    "    # Note: that this line can be augmented and used over\n",
    "    # to replace any characters with nothing or a space\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans(' ', ' ', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    useless_words = useless_words + ['hi', 'im']\n",
    "\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    text_filtered = nlp(' '.join(text_filtered))\n",
    "    text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that reads text file\n",
    "def read_file(filepath: str) -> str:\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "usyd_test_subject = read_file(f\"./data/subjects/usyd/COMP2017.txt\")\n",
    "uts_test_subject = read_file(f\"./data/subjects/uts/31242.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_section(text: str, start_line: str, end_line: str) -> str:\n",
    "  description = \"\"\n",
    "  is_description = False\n",
    "  \n",
    "  for line in text.split(\"\\n\"):\n",
    "    if is_description:\n",
    "      if line.strip() == \"\":\n",
    "        continue\n",
    "      \n",
    "      if line == end_line:\n",
    "        break\n",
    "      description += line + \"\\n\"\n",
    "      continue\n",
    "    \n",
    "    if line == start_line:\n",
    "      is_description = True\n",
    "      \n",
    "  return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "usyd_description = get_text_section(usyd_test_subject, \"2024 unit information\", \"Unit details and rules\")\n",
    "uts_description = get_text_section(uts_test_subject, \"Description\", \"Subject learning objectives (SLOs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "usyd_slos = get_text_section(usyd_test_subject, \"At the completion of this unit, you should be able to:\", \"Unit availability\")\n",
    "uts_slos = get_text_section(uts_test_subject, \"Subject learning objectives (SLOs)\", \"Course intended learning outcomes (CILOs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "uts_test_subject = clean_string(uts_description + \"\\n\\n\" +  uts_slos)\n",
    "usyd_test_subject = clean_string(usyd_description +  \"\\n\\n\" + usyd_slos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(uts_test_subject)\n",
    "phrase_rank = [(phrase.text, phrase.rank) for phrase in doc._.phrases]\n",
    "phrase_rank.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transaction use web application', 0.12317288565236939),\n",
       " ('sophisticated web application deployment production subject',\n",
       "  0.11676620401850937),\n",
       " ('medium sized web application', 0.1156558144462636),\n",
       " ('feature web base application system', 0.11348758388087492),\n",
       " ('student practice internet programming', 0.11215215625442283),\n",
       " ('contrast compete web application architecture list advantage disadvantage',\n",
       "  0.10650539421645219),\n",
       " ('conceptual level multitier distribute web application component technology use',\n",
       "  0.10488159375068266),\n",
       " ('multiple datum source transaction integrity datum application security',\n",
       "  0.10270438163402332),\n",
       " ('web development stack ntier architecture standard transaction security dependency injection layering webservice integration deployment subject run simulation technologybased startup house software development project student',\n",
       "  0.10189592735763967),\n",
       " ('successful completion subject student', 0.10058137723993472)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_rank[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(usyd_test_subject)\n",
    "phrase_rank = [(phrase.text, phrase.rank, list(phrase.chunks)) for phrase in doc._.phrases]\n",
    "phrase_rank.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('common unix tool manage aspect software construction process version control regression testing subject',\n",
       "  0.0940700251393751,\n",
       "  [common unix tool manage aspect software construction process version control regression testing subject]),\n",
       " ('standard link list datum structure high performance',\n",
       "  0.0888826382464698,\n",
       "  [standard link list datum structure high performance]),\n",
       " ('concurrent thread debugging tool technique',\n",
       "  0.08799629729135139,\n",
       "  [concurrent thread debugging tool technique]),\n",
       " ('error example code fixing use debugger',\n",
       "  0.08631106180335711,\n",
       "  [error example code fixing use debugger]),\n",
       " ('common programming error', 0.08616693776133381, [common programming error]),\n",
       " ('common memoryrelate error memory leak dangle pointer',\n",
       "  0.08581231800785805,\n",
       "  [common memoryrelate error memory leak dangle pointer]),\n",
       " ('high performance',\n",
       "  0.08498104678511194,\n",
       "  [high performance, high performance]),\n",
       " ('handle high performance', 0.08388062703009652, [handle high performance]),\n",
       " ('tool philosophy process', 0.08321349783128315, [tool philosophy process]),\n",
       " ('unix environment specific code topic',\n",
       "  0.08071902583603985,\n",
       "  [unix environment specific code topic])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_rank[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_keywords(doc, n):\n",
    "    keywords = {}\n",
    "  \n",
    "    for phrase in doc._.phrases:\n",
    "        phrase_text = phrase.text\n",
    "        \n",
    "        for keyword in phrase_text.split(\" \"):\n",
    "          keywords[keyword] = keywords.get(keyword, 0) + 1\n",
    "          \n",
    "    if \"\" in keywords:\n",
    "        del keywords[\"\"]\n",
    "          \n",
    "    return [word[0] for word in sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25948dde08164c15addfd8f05525b1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing uts subjects:   0%|          | 0/265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad20599223c24cbfa47d592e4a81803c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing usyd subjects:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subject_code_to_keywords = {}\n",
    "\n",
    "for uni in [\"uts\", \"usyd\"]:  \n",
    "  for filename in tqdm(os.listdir(f\"./data/subjects/{uni}\"), desc=f\"Processing {uni} subjects\"):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "      continue\n",
    "    \n",
    "    filepath = f\"./data/subjects/{uni}/{filename}\"\n",
    "    text = read_file(filepath)\n",
    "    \n",
    "    description = None\n",
    "    slos = None\n",
    "    \n",
    "    if uni == \"uts\":\n",
    "      description = get_text_section(text, \"Description\", \"Subject learning objectives (SLOs)\")\n",
    "      slos = get_text_section(text, \"Subject learning objectives (SLOs)\", \"Course intended learning outcomes (CILOs)\")\n",
    "    else:\n",
    "      description = get_text_section(text, \"2024 unit information\", \"Unit details and rules\")\n",
    "      slos = get_text_section(text, \"At the completion of this unit, you should be able to:\", \"Unit availability\")\n",
    "    \n",
    "    relevant_text = clean_string(description + \" \" + slos)\n",
    "    \n",
    "    doc = nlp(relevant_text)\n",
    "    keywords = get_top_n_keywords(doc, 20)\n",
    "    subject_code_to_keywords[filename.replace(\".txt\", \"\")] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/subjects/subject_keywords.json\", \"w\") as f:\n",
    "    json.dump(subject_code_to_keywords, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
